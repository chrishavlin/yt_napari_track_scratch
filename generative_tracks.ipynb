{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6caac0a-ca39-433b-8450-f9f76aaa57fb",
   "metadata": {},
   "source": [
    "using the napari tracks layers : https://napari.org/stable/howtos/layers/tracks.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294e0e31-9e70-485f-bb86-e552d1bb5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2b8ae0-6914-41cf-9768-3fcd6e6a5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f925714-5950-416c-8fc7-b866510d6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1182e243-405d-4002-a9a5-b832573e47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tracks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtail_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtail_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhead_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtranslate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'additive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolor_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'track_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolormaps_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexperimental_clipping_planes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnapari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add a Tracks layer to the layer list. \n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : array (N, D+1)\n",
       "    Coordinates for N points in D+1 dimensions. ID,T,(Z),Y,X. The first\n",
       "    axis is the integer ID of the track. D is either 3 or 4 for planar\n",
       "    or volumetric timeseries respectively.\n",
       "features : Dataframe-like\n",
       "    Features table where each row corresponds to a point and each column\n",
       "    is a feature.\n",
       "properties : dict {str: array (N,)}, DataFrame\n",
       "    Properties for each point. Each property should be an array of length N,\n",
       "    where N is the number of points.\n",
       "graph : dict {int: list}\n",
       "    Graph representing associations between tracks. Dictionary defines the\n",
       "    mapping between a track ID and the parents of the track. This can be\n",
       "    one (the track has one parent, and the parent has >=1 child) in the\n",
       "    case of track splitting, or more than one (the track has multiple\n",
       "    parents, but only one child) in the case of track merging.\n",
       "    See examples/tracks_3d_with_graph.py\n",
       "color_by : str\n",
       "    Track property (from property keys) by which to color vertices.\n",
       "tail_width : float\n",
       "    Width of the track tails in pixels.\n",
       "tail_length : float\n",
       "    Length of the positive (backward in time) tails in units of time.\n",
       "head_length : float\n",
       "    Length of the positive (forward in time) tails in units of time.\n",
       "colormap : str\n",
       "    Default colormap to use to set vertex colors. Specialized colormaps,\n",
       "    relating to specified properties can be passed to the layer via\n",
       "    colormaps_dict.\n",
       "colormaps_dict : dict {str: napari.utils.Colormap}\n",
       "    Optional dictionary mapping each property to a colormap for that\n",
       "    property. This allows each property to be assigned a specific colormap,\n",
       "    rather than having a global colormap for everything.\n",
       "name : str\n",
       "    Name of the layer.\n",
       "metadata : dict\n",
       "    Layer metadata.\n",
       "scale : tuple of float\n",
       "    Scale factors for the layer.\n",
       "translate : tuple of float\n",
       "    Translation values for the layer.\n",
       "rotate : float, 3-tuple of float, or n-D array.\n",
       "    If a float convert into a 2D rotation matrix using that value as an\n",
       "    angle. If 3-tuple convert into a 3D rotation matrix, using a yaw,\n",
       "    pitch, roll convention. Otherwise assume an nD rotation. Angles are\n",
       "    assumed to be in degrees. They can be converted from radians with\n",
       "    np.degrees if needed.\n",
       "shear : 1-D array or n-D array\n",
       "    Either a vector of upper triangular values, or an nD shear matrix with\n",
       "    ones along the main diagonal.\n",
       "affine : n-D array or napari.utils.transforms.Affine\n",
       "    (N+1, N+1) affine transformation matrix in homogeneous coordinates.\n",
       "    The first (N, N) entries correspond to a linear transform and\n",
       "    the final column is a length N translation vector and a 1 or a napari\n",
       "    `Affine` transform object. Applied as an extra transform on top of the\n",
       "    provided scale, rotate, and shear values.\n",
       "opacity : float\n",
       "    Opacity of the layer visual, between 0.0 and 1.0.\n",
       "blending : str\n",
       "    One of a list of preset blending modes that determines how RGB and\n",
       "    alpha values of the layer visual get mixed. Allowed values are\n",
       "    {'opaque', 'translucent', and 'additive'}.\n",
       "visible : bool\n",
       "    Whether the layer visual is currently being displayed.\n",
       "cache : bool\n",
       "    Whether slices of out-of-core datasets should be cached upon retrieval.\n",
       "    Currently, this only applies to dask arrays.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "layer : :class:`napari.layers.Tracks`\n",
       "    The newly-created tracks layer.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.10.11/envs/yt-napari/lib/python3.10/site-packages/napari/components/viewer_model.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v.add_tracks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a201f1-0706-4973-a4f0-8f8add99d611",
   "metadata": {},
   "source": [
    "## biased random walk \n",
    "\n",
    "The following function takes a fixed number of particles and projects forward in time using a biased random walk. At each timestep, the trajectory is a weighted average of the previous timestep's trajectory with a new trajectory (of random orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3af0d4-2e27-442d-877e-a58a70b8c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_random_walk_tracks(n_particles, n_timesteps, image_size_xyz, bias_old = 0.8):    \n",
    "    # random walk with biases towards trajectory from prior timestep. particles are \n",
    "    # individual and persistent \n",
    "    track_data = []\n",
    "    ids = np.arange(n_particles, dtype='int').transpose()\n",
    "    \n",
    "    # initialize start positions     \n",
    "    xyz = np.random.random((n_particles, 3))\n",
    "    t = np.zeros((n_particles,))\n",
    "\n",
    "    # track array columns: ID,T,(Z),Y,X\n",
    "    current_track = np.column_stack([ids, t, xyz[:, 2], xyz[:, 1], xyz[:, 0]])\n",
    "    track_data.append(current_track)      \n",
    "    \n",
    "    for it in range(n_timesteps):\n",
    "        d_xyz = (np.random.random((n_particles, 3)) - 0.5) * 0.1\n",
    "\n",
    "        if it > 0:\n",
    "            d_xyz = d_xyz0 * bias_old + d_xyz * (1 - bias_old)\n",
    "        \n",
    "        xyz = xyz + d_xyz \n",
    "        t = t + 1 # arbitrary time\n",
    "        current_track = np.column_stack([ids, t, xyz[:, 2], xyz[:, 1], xyz[:, 0]])\n",
    "        track_data.append(current_track)\n",
    "        \n",
    "        d_xyz0 = d_xyz\n",
    "\n",
    "    track_data = np.concatenate(track_data)\n",
    "\n",
    "    # scale final position ranges\n",
    "    for idim in range(3): \n",
    "        mx_dim = track_data[:, idim+2].max()\n",
    "        mn_dim = track_data[:, idim+2].min()\n",
    "        scaled = (track_data[:, idim+2] - mn_dim)/ (mx_dim - mn_dim)\n",
    "        track_data[:, idim+2] = scaled * image_size_xyz[idim]\n",
    "\n",
    "    track_data = track_data[track_data[:, 0].argsort()]#.astype('int')\n",
    "    \n",
    "    return track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebd8373-c3c5-4b01-b5a2-524503ccd6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10050, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_particles = 50\n",
    "timesteps = 200\n",
    "tracks = biased_random_walk_tracks(n_particles, timesteps, (400, 400, 400))\n",
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80520b8-4cc9-4a1b-8339-50b9f125f83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6424e410-bbea-44e3-b11d-7d5653681dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tracks layer 'tracks' at 0x2be5d7f70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.add_tracks(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900b78-1ec2-4e0d-bfdb-dbab3fdabb0e",
   "metadata": {},
   "source": [
    "notes on viewing in napari:\n",
    "\n",
    "in 2D view, all tracks are projected to the 2d plane (https://github.com/napari/napari/issues/3861)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec4cfd-59d8-4142-b1d5-0d434c848fda",
   "metadata": {},
   "source": [
    "## biased random walk with particle generation \n",
    "\n",
    "Same as before, but now there's a nonzero chance of each particle spawning a new particle. The new particle starts with the same position and old trajectory (which will be weighted with a new, different trajectory)\n",
    "\n",
    "Still just using `np.random.random`.... a new particle is generated if `np.random.random() < gen_particle_coeff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219898dc-1818-419b-936d-ce208c17aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_random_walk_tracks_with_generation(n_starting, \n",
    "                                              n_max_particles, \n",
    "                                              n_timesteps, \n",
    "                                              image_size_xyz, \n",
    "                                              bias_old = 0.8,\n",
    "                                              gen_particle_coeff=0.05):    \n",
    "    # random walk with biases towards trajectory from prior timestep. particles are \n",
    "    # individual and persistent \n",
    "    track_data = []\n",
    "    \n",
    "    # initialize start positions     \n",
    "    xyz = np.random.random((n_starting, 3))\n",
    "    t = np.zeros((n_starting,))\n",
    "    ids = np.arange(n_starting, dtype='int').transpose()\n",
    "\n",
    "    # track array columns: ID,T,(Z),Y,X\n",
    "    current_track = np.column_stack([ids, t, xyz[:, 2], xyz[:, 1], xyz[:, 0]])\n",
    "    track_data.append(current_track)\n",
    "\n",
    "    n_particles = n_starting\n",
    "    graph = {}\n",
    "    for it in range(n_timesteps):\n",
    "\n",
    "        # particle generation\n",
    "        generate_draw = np.random.random((n_particles,)) < gen_particle_coeff\n",
    "        n_new_particles = np.sum(generate_draw)\n",
    "        \n",
    "        if n_new_particles  > 0 and n_particles < n_max_particles:\n",
    "            # graph : dict {int: list}\n",
    "            # Graph representing associations between tracks. Dictionary defines the\n",
    "            # mapping between a track ID and the parents of the track. This can be\n",
    "            # one (the track has one parent, and the parent has >=1 child) in the\n",
    "            # case of track splitting, or more than one (the track has multiple\n",
    "            # parents, but only one child) in the case of track merging.\n",
    "            # See examples/tracks_3d_with_graph.py\n",
    "            parent_ids = ids[generate_draw]\n",
    "            new_ids = np.arange(len(ids), len(ids)+n_new_particles, dtype='int').transpose()\n",
    "            for id, parent_id in zip(new_ids, parent_ids):\n",
    "                # child points to parent\n",
    "                graph[id] = parent_id\n",
    "                            \n",
    "            ids = np.concatenate([ids, new_ids])\n",
    "            xyz = np.concatenate([xyz, xyz[generate_draw]])            \n",
    "            t = np.concatenate([t, t[generate_draw]])                        \n",
    "            n_particles = len(ids)\n",
    "            if it > 0: \n",
    "                d_xyz0 = np.concatenate([d_xyz0, d_xyz0[generate_draw]])\n",
    "            \n",
    "        \n",
    "        d_xyz = (np.random.random((n_particles, 3)) - 0.5) * 0.1\n",
    "\n",
    "        if it > 0:\n",
    "            d_xyz = d_xyz0 * bias_old + d_xyz * (1 - bias_old)\n",
    "        \n",
    "        xyz = xyz + d_xyz \n",
    "        t = t + 1 # arbitrary time\n",
    "        current_track = np.column_stack([ids, t, xyz[:, 2], xyz[:, 1], xyz[:, 0]])\n",
    "        track_data.append(current_track)\n",
    "        \n",
    "        d_xyz0 = d_xyz\n",
    "\n",
    "    track_data = np.concatenate(track_data)\n",
    "\n",
    "    # scale final position ranges\n",
    "    for idim in range(3): \n",
    "        mx_dim = track_data[:, idim+2].max()\n",
    "        mn_dim = track_data[:, idim+2].min()\n",
    "        scaled = (track_data[:, idim+2] - mn_dim)/ (mx_dim - mn_dim)\n",
    "        track_data[:, idim+2] = scaled * image_size_xyz[idim]\n",
    "\n",
    "    track_data = track_data[track_data[:, 0].argsort()]#.astype('int')\n",
    "    \n",
    "    return track_data, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aac942c-c4dc-418c-a7b7-517d0b3cd3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9819, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_particles = 2\n",
    "n_max_particles = 100\n",
    "timesteps = 200\n",
    "tracks, graph = biased_random_walk_tracks_with_generation(n_particles, \n",
    "                                                   n_max_particles,\n",
    "                                                   timesteps, \n",
    "                                                   (400, 400, 400), \n",
    "                                                    gen_particle_coeff=0.03,\n",
    "                                                    )\n",
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f85fa-e770-42af-adef-bcccb98f98fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a131204b-ef81-45fa-861b-75fc2f229c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tracks layer 'tracks' at 0x2be5a71f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.layers.clear()\n",
    "v.add_tracks(tracks, graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12090a3-1996-4d2d-9393-1a87cb0c0aba",
   "metadata": {},
   "source": [
    "## biased random walk with annihilation \n",
    "\n",
    "* particles falling within specified distance collapse into 1 new survivor\n",
    "* fixed, sticky boundary: particles projected past box edges are moved back, just a simple way to increase interactions (reflective boundary would be nicer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d004f042-3f5f-4cc3-8cbe-e1bb397cea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def biased_random_walk_tracks_with_annihilation(n_starting,                                               \n",
    "                                              n_timesteps, \n",
    "                                              image_size_xyz, \n",
    "                                              bias_old = 0.8,\n",
    "                                              annihilation_dist=0.05):    \n",
    "    # random walk with biases towards trajectory from prior timestep. particles are \n",
    "    # individual and persistent \n",
    "    track_data = []\n",
    "    \n",
    "    # initialize start positions     \n",
    "    xyz = np.random.random((n_starting, 3))\n",
    "    t = np.zeros((n_starting,))\n",
    "    ids = np.arange(n_starting, dtype='int').transpose()\n",
    "    surviving = ids >= 0\n",
    "    # track array columns: ID,T,(Z),Y,X\n",
    "    current_track = np.column_stack([ids, t, xyz[:, 2], xyz[:, 1], xyz[:, 0]])\n",
    "    track_data.append(current_track)\n",
    "\n",
    "    n_particles = n_starting\n",
    "    graph = {}\n",
    "    for it in range(n_timesteps):\n",
    "        # print(f\"{len(ids[surviving])} remain\")\n",
    "        # find points within annihilation_dist of each other. this is hella slow and ugly but whatever        \n",
    "        ids_to_rm = []\n",
    "        survivors = []\n",
    "        survivor_parents = {}\n",
    "        for id in ids[surviving]:            \n",
    "            if id not in ids_to_rm: \n",
    "                dist = np.zeros(ids.shape)\n",
    "                for idim in range(3):\n",
    "                    dist += (xyz[:, idim] - xyz[id, idim])**2\n",
    "                dist = np.sqrt(dist)                \n",
    "                close_enough = (dist < annihilation_dist) & (surviving)                \n",
    "                if np.sum(close_enough) > 1: \n",
    "                    close_ids = ids[close_enough]\n",
    "                    survivors.append(id)\n",
    "                    survivor_parents[id] = [idc for idc in close_ids]\n",
    "                    ids_to_rm += [idc for idc in close_ids]\n",
    "\n",
    "        # add particles for the survivor\n",
    "        surviving[ids_to_rm] = False \n",
    "        new_particles = len(survivors)\n",
    "        if new_particles > 0:\n",
    "            new_ids = []            \n",
    "        \n",
    "            new_id = len(ids) - 1\n",
    "            for survivor in survivors:\n",
    "                new_id += 1\n",
    "                new_ids.append(new_id)\n",
    "                \n",
    "                graph[new_id] = survivor_parents[survivor]\n",
    "\n",
    "            ids = np.concatenate([ids, new_ids])\n",
    "            xyz = np.concatenate([xyz, xyz[survivors,:]])            \n",
    "            t = np.concatenate([t, t[survivors]])                        \n",
    "            surviving = np.concatenate([surviving, [True,] * len(new_ids)])  \n",
    "            n_particles = len(ids)\n",
    "            if it > 0: \n",
    "                d_xyz0 = np.concatenate([d_xyz0, d_xyz0[survivors,:]])\n",
    " \n",
    "        d_xyz = (np.random.random((n_particles, 3)) - 0.5) * 0.1\n",
    "\n",
    "        if it > 0:\n",
    "            d_xyz = d_xyz0 * bias_old + d_xyz * (1 - bias_old)\n",
    "        \n",
    "        xyz = xyz + d_xyz\n",
    "\n",
    "        # pin to boundary (avoid periodic tail effect)\n",
    "        exited_right = xyz > 1.0 \n",
    "        exited_left = xyz < 0.0\n",
    "        xyz[exited_right] = 1.0 \n",
    "        xyz[exited_left] = 0.0\n",
    "        \n",
    "        t = t + 1 # arbitrary time\n",
    "        current_track = np.column_stack([ids[surviving], t[surviving], xyz[surviving, 2], xyz[surviving, 1], xyz[surviving, 0]])\n",
    "        track_data.append(current_track)\n",
    "        \n",
    "        d_xyz0 = d_xyz\n",
    "\n",
    "    track_data = np.concatenate(track_data)\n",
    "\n",
    "    # scale final position ranges\n",
    "    for idim in range(3): \n",
    "        mx_dim = track_data[:, idim+2].max()\n",
    "        mn_dim = track_data[:, idim+2].min()\n",
    "        scaled = (track_data[:, idim+2] - mn_dim)/ (mx_dim - mn_dim)\n",
    "        track_data[:, idim+2] = scaled * image_size_xyz[idim]\n",
    "\n",
    "    track_data = track_data[track_data[:, 0].argsort()]#.astype('int')\n",
    "    \n",
    "    return track_data, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afd9eba-66ff-4d22-8614-31c0c6d7abbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37478, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_particles = 1000\n",
    "timesteps = 200\n",
    "tracks, graph = biased_random_walk_tracks_with_annihilation(n_particles,                                                    \n",
    "                                                   timesteps, \n",
    "                                                   (400, 400, 400), \n",
    "                                                    annihilation_dist = 0.05\n",
    "                                                    )\n",
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90961447-531e-4467-a989-593c56e96c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tracks layer 'tracks' at 0x2c5b96f50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.layers.clear()\n",
    "v.add_tracks(tracks, graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880eba4-00bb-4eb8-9436-0690e274ed92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca0fa3-a640-413c-a1f5-9fc03b47d136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece0d4b-4a67-45b8-968b-03b7f8d816a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c286b54-c731-4250-af5e-04423df97ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4aa88-a2fd-4104-b578-ff24b5b09ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
